version: '3.8'

x-airflow-common:
  &airflow-common
  build: .
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    _PIP_ADDITIONAL_REQUIREMENTS: ''
    # Auto-create critical connections
    AIRFLOW_CONN_POSTGRES_DEFAULT: postgresql://airflow:airflow@postgres:5432/airflow
    AIRFLOW_CONN_MSSQL_CRM: mssql+pymssql://sa:YourStrong!Passw0rd@mssql_crm:1433
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./dbt:/opt/dbt
    - ./dbt/profiles.yml:/opt/airflow/.dbt/profiles.yml
  depends_on:
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy
    trino:
      condition: service_healthy

services:
  producer_service:
    build:
      context: ./producer_service
    container_name: producer_service
    environment:
      - KAFKA_BROKER=kafka-1:9092,kafka-2:9092,kafka-3:9092
      - KAFKA_TOPIC=etl_events
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
      - kafka-init
    ports:
      - "8000:8000"

  consumer_service:
    build:
      context: ./consumer_service
    # container_name kaldırıldı - scale için gerekli!
    environment:
      - KAFKA_BROKER=kafka-1:9092,kafka-2:9092,kafka-3:9092
      - KAFKA_TOPIC=etl_events
      - KAFKA_CONSUMER_GROUP=etl_group  # Aynı group'ta olmalılar!
      - MINIO_ENDPOINT=object-store:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
      - MINIO_BUCKET=etl-raw
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
      - kafka-init
      - object-store
    # Port kaldırıldı - scale için çakışmasın
    # API endpoint'leri içeriden erişilebilir
    deploy:
      replicas: 3  # 3 instance çalışsın!
  nginx:
    image: nginx:latest
    container_name: nginx_gateway
    ports:
      - "8080:80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      - dbt-service
     
  dbt-service:
    build:
      context: ./dbt_service
    container_name: dbt_service
    environment:
      - DBT_PROJECT_DIR=/opt/dbt
    volumes:
      - ./dbt:/opt/dbt
    depends_on:
      - postgres
    ports:
      - "8003:8003"
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  mssql:
    image: mcr.microsoft.com/mssql/server:2019-latest
    container_name: mssql_crm
    user: "0:0"
    environment:
      - ACCEPT_EULA=Y
      - SA_PASSWORD=YourStrong!Passw0rd
      - MSSQL_PID=Developer
    ports:
      - "1434:1433"
    volumes:
      - mssql-data:/var/opt/mssql
    healthcheck:
      test: /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P YourStrong!Passw0rd -Q "SELECT 1" || exit 1
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 10s
    restart: always

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    restart: always

 
  kafka-1:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
    restart: always
    volumes:
      - kafka-1-data:/var/lib/kafka/data

  
  kafka-2:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-2
    depends_on:
      - zookeeper
    ports:
      - "9093:9092"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
    restart: always
    volumes:
      - kafka-2-data:/var/lib/kafka/data

  
  kafka-3:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-3
    depends_on:
      - zookeeper
    ports:
      - "9094:9092"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
    restart: always
    volumes:
      - kafka-3-data:/var/lib/kafka/data

  
  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-init
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
    entrypoint: ['/bin/bash', '-c']
    command: |
      "
      echo 'Waiting for Kafka brokers to be ready...'
      sleep 10
      
      echo 'Creating topics with partitions...'
      
      # Topic 1: etl_events (3 partition, 3 replica)
      kafka-topics --create --if-not-exists \
        --topic etl_events \
        --partitions 3 \
        --replication-factor 3 \
        --bootstrap-server kafka-1:9092,kafka-2:9092,kafka-3:9092
      
      # Topic 2: crm_data (5 partition, 3 replica)
      kafka-topics --create --if-not-exists \
        --topic crm_data \
        --partitions 5 \
        --replication-factor 3 \
        --bootstrap-server kafka-1:9092,kafka-2:9092,kafka-3:9092
      
      # Topic 3: erp_data (5 partition, 3 replica)
      kafka-topics --create --if-not-exists \
        --topic erp_data \
        --partitions 5 \
        --replication-factor 3 \
        --bootstrap-server kafka-1:9092,kafka-2:9092,kafka-3:9092
      
      echo 'Listing all topics:'
      kafka-topics --list --bootstrap-server kafka-1:9092
      
      echo 'Topic details:'
      kafka-topics --describe --bootstrap-server kafka-1:9092
      
      echo 'Kafka topics initialized successfully!'
      "
    restart: on-failure


  redis:
    image: redis:latest
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always

  trino:
    image: trinodb/trino:latest
    container_name: trino
    ports:
      - "8082:8080"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/v1/info || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      - postgres

  object-store:
    image: minio/minio:latest
    container_name: minio
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - minio-data:/data
    ports:
      - "9011:9001"
      - "9010:9000"
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8081:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        exec /entrypoint airflow version
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: airflow
      _AIRFLOW_WWW_USER_PASSWORD: airflow
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"
    volumes:
      - .:/sources

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    command:
      - bash
      - -c
      - airflow

  flower:
    <<: *airflow-common
    command: celery flower
    profiles:
      - flower
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always

volumes:
  postgres-db-volume:
  minio-data:
  kafka-1-data:
  kafka-2-data:
  kafka-3-data:
  mssql-data: